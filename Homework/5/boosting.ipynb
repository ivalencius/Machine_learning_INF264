{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting algorithm\n",
    "\n",
    "\n",
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "dataset = pd.read_csv('sonar.all-data',header=None)\n",
    "X = dataset.iloc[:,:-1].to_numpy()\n",
    "# Labels should be -1 and 1!\n",
    "y = (dataset.iloc[:,-1].to_numpy()=='M').astype(int)\n",
    "y = np.where(y==0, -np.ones_like(y), y)\n",
    "print(\"Number of samples: \", X.shape[0])\n",
    "print('Number of features: ', X.shape[-1])\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "### Boosting train and predict functions\n",
    "\n",
    "\n",
    "**Note 1 **: \n",
    "\n",
    "in scikit-learn, all supervised estimators implement a ``fit(X, y)`` method and a ``predict(X)`` method with ``X`` being unlabeled observations and  ``y`` being labels. \n",
    "\n",
    "Therefore ``Classifier`` parameter can be any sklearn class implementing a supervised classifier.\n",
    "\n",
    "(See *The problem solved in supervised learning* section in the supervised learning tutorial from [sklearn documentation](https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html)\n",
    "\n",
    "**Note 2 **: \n",
    "\n",
    "Some sklearn classifiers (such as [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), etc.)  have a ``sample_weight`` parameters in their ``fit`` and ``score`` methods, making it easy to implement a user-defined boosting algorithm. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting_train(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    n_clfs,           # number of classifier\n",
    "    Classifier,       # Python class of classifier\n",
    "    clfs_args = {},   # Specific python class of classifier's arguments\n",
    "):\n",
    "    \"\"\"\n",
    "    Adaboost training lgorithm\n",
    "    \"\"\"\n",
    "    clfs = []\n",
    "    alphas = []\n",
    "\n",
    "    n = len(X_train) \n",
    "    # Initialize weights to 1/n\n",
    "    ... #TODO!\n",
    "    for t in range(n_clfs):\n",
    "        # -------------------------\n",
    "        # Train a new classifier\n",
    "        # -------------------------\n",
    "\n",
    "        # Train a weak learner using the training data and the sample weights\n",
    "        ... #TODO!\n",
    "        # Compute weighted training error \n",
    "        ... #TODO!\n",
    "        # Compute alpha_t (and avoid math errors)\n",
    "        ... #TODO!\n",
    "\n",
    "        # -------------------------\n",
    "        # Update weights\n",
    "        # -------------------------\n",
    "        \n",
    "        ... #TODO!\n",
    "\n",
    "    # Return the list of trained classifiers composing the boosting classifier\n",
    "    # with their corresponding weights 'alphas'\n",
    "    return(clfs, alphas)\n",
    "\n",
    "def boosting_predict(\n",
    "    clfs,       # list of classifiers composing the boosting classifier\n",
    "    alphas,     # Weights associated with each classifier in 'clfs'\n",
    "    X_test,\n",
    "):\n",
    "    \"\"\"\n",
    "    Adaboost predict algorithm\n",
    "    \"\"\"\n",
    "    ... #TODO! \n",
    "    return(y_pred)"
   ]
  },
  {
   "source": [
    "### Cross validation\n",
    "\n",
    "\n",
    "You can use the [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) from sklearn to split your datasets into k folds. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold_split(X, y, num_folds, seed=int(666)):\n",
    "    \"\"\"\n",
    "    Split 'X' and 'y' into k-folds with k='num_fold'\n",
    "    \"\"\"\n",
    "    KFold_splitter = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "    # Initialize lists of X (train/val) and y (train/val) for each fold\n",
    "    X_train_folds = []  \n",
    "    X_val_folds = []\n",
    "    y_train_folds = []\n",
    "    y_val_folds = []\n",
    "    for (kth_fold_train_idxs, kth_fold_val_idxs) in KFold_splitter.split(X, y):\n",
    "        X_train_folds.append(X[kth_fold_train_idxs])\n",
    "        X_val_folds.append(X[kth_fold_val_idxs])\n",
    "        y_train_folds.append(y[kth_fold_train_idxs])\n",
    "        y_val_folds.append(y[kth_fold_val_idxs])\n",
    "    # Return the list of k-folds datasets\n",
    "    return X_train_folds, X_val_folds, y_train_folds, y_val_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_boosting(\n",
    "    X_train_val, \n",
    "    y_train_val, \n",
    "    num_folds, \n",
    "    n_clfs,                                    # number of classifier\n",
    "    Classifier = DecisionTreeClassifier,       # Python class of classifier\n",
    "    clfs_args = {\"max_depth\" : 1},             # Specific python class of classifier's arguments\n",
    "    seed=int(666),\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a cross validation of the boosting algorithm\n",
    "    \"\"\"\n",
    "    ... #TODO!\n",
    "    return boosting_clfs, boosting_alphas, boosting_scores"
   ]
  },
  {
   "source": [
    "### Test boosting on the sonar dataset\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "# Extract a test set:\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.35, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# For each hyper-parameter instance, do KFold cross validation:\n",
    "for n_clfs in [1, 10, 50, 100, 200, 300, 400]:\n",
    "    boosting_clfs, boosting_alphas, boosting_scores = evaluate_boosting(...)#TODO!\n",
    "    print('Trees: %d' % n_clfs)\n",
    "    print('Validation scores: ', [round(s, 3) for s in boosting_scores])\n",
    "    print('Mean validation accuracy: %.3f' % (sum(boosting_scores)/len(boosting_scores)))\n",
    "    # Test accuracy computed with the boosting classifier trained with the first k-fold\n",
    "    test_acc = accuracy_score(\n",
    "        boosting_predict(boosting_clfs[0], boosting_alphas[0], X_test),\n",
    "        y_test,\n",
    "    )\n",
    "    print('Test set accuracy: %.3f' %test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}