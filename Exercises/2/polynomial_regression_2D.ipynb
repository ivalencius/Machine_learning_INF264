{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#%matplotlib notebook\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib.colors import ListedColormap\r\n",
    "from mpl_toolkits import mplot3d\r\n",
    "from matplotlib import cm\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "from sklearn.datasets import load_boston\r\n",
    "from sklearn.preprocessing import PolynomialFeatures\r\n",
    "from sklearn.linear_model import LinearRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the last exercise of this group session, we propose to look at the problem of polynomial regression on real world data. We will consider the Boston Housing dataset, in which each sample corresponds to a house, whose price is to be infered from a set of 13 features. The purpose of this exercise is to:\n",
    "- Show that linear models can be used successfuly on real world problems.\n",
    "- Use linear models in a multi-dimentional setting.\n",
    "- Get familiar with visualization in the presence of 2 dimensions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading and visualizing the Boston Housing dataset\n",
    "\n",
    "1) Load the Boston dataset: store the 13 features in a matrix X and the target price in a vector Y.\n",
    "\n",
    "2) Investigate the Boston dataset. It is obviously not possible to visualize all the 14 dimensions at the same time, but it may be a good idea to represent the target price as a function of each of the 13 features individually. Hint: To load the dataset, use the 'load_boston()' from 'sklearn.datasets'. You can plot using the 'scatter' function from 'matplotlib.pyplot'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model selection for polynomial regression on the Boston dataset: 2D case\r\n",
    "\r\n",
    "3) Identify 2 features that are continuous and appear to be correlated to the target price. Extract these two features to obtain a simpler features matrix 'X_2D' with only 2 columns.\r\n",
    "\r\n",
    "4) Split and shuffle the dataset into train/validation/test sets. Specify the proportion for each. You can for instance use the 'train_test_split()' function from 'sklearn.model_selection'.\r\n",
    "\r\n",
    "5) Perform a model selection on the degree parameter for the polynomial regression model: in a For loop with varying degree 'd', train and validate a polynomial regression model of degree 'd' on your dataset. You can for instance transform your data in polynomial form using the class 'PolynomialFeatures()' from 'sklearn.preprocessing', then use a linear model using the class 'LinearRegression()' from 'sklearn.linear_model'. Select the best model as the model whose mean-squared error (MSE) is the least when evaluated on the VALIDATION set.\r\n",
    "\r\n",
    "6) Perform a model evaluation of the selected model: train the best model on both the train and the validation sets, then evaluate it on the test set. Note: the test set is only used at the very end of your \"pipeline\" (AFTER model selection) to assess the ability of the selected model to generalize on UNSEEN data. You should never use the validation set for this purpose, since the validation set was already used to select the best model, thus it is no longer unseen !\r\n",
    "\r\n",
    "7) In the For loop from 5), store both the train and validation MSE values for each degree. Plot the train and validation MSE as functions of the degree. Comment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fixing the seed for reproducibility:\r\n",
    "seed = 222\r\n",
    "# 3) Selecting two continuous features that are correlated to the target price:\r\n",
    "X_2D = ...\r\n",
    "# 4) Splitting and shuffling dataset into train/validation/test sets:\r\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = ...\r\n",
    "# 5) Model selection on the degree in polynomial regression:\r\n",
    "degrees = [... # choose a bunch of interesting values for d]\r\n",
    "val_MSEs = []\r\n",
    "for degree in degrees:\r\n",
    "    val_MSE = ... # TODO: train on the train set then evaluate on the validation set a\r\n",
    "                  # polynomial regression model of degree d\r\n",
    "    val_MSEs.append(val_MSE)\r\n",
    "# Extracting the best model:\r\n",
    "val_MSEs = np.array(val_MSEs)\r\n",
    "print(\"Validation MSEs:\")  \r\n",
    "print(val_MSEs)\r\n",
    "best_degree_idx = ...\r\n",
    "best_degree = ...\r\n",
    "print(\"\\nBest degree:\", best_degree)\r\n",
    "# 6) Model evaluation:\r\n",
    "... # TODO: evaluate on the test set the selected model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization of underfitting and overfitting\n",
    "\n",
    "8) Read carefully the functions below. These functions let you visualize 3D plots representing the true data points and the prediction surfaces. This is interesting in order to visually assess whether a model underfits or overfits. Use the 'visualize_2D_polynomial_regression()' function in order to assess which degree values tend to underfit or overfit for your polynomial regression models on the Boston dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_axis(X, step=0.1):\r\n",
    "    X_min = np.min(X)\r\n",
    "    X_max = np.max(X)\r\n",
    "    X_axis = np.arange(X_min, X_max, step)\r\n",
    "    return X_axis"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_grid(X0, X1):\r\n",
    "    X0_axis = make_axis(X0)\r\n",
    "    X1_axis = make_axis(X1)\r\n",
    "    X0_grid, X1_grid = np.meshgrid(X0_axis, X1_axis)\r\n",
    "    return X0_grid, X1_grid"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def predict_grid(model, poly_features, X0_grid, X1_grid):\r\n",
    "    original_shape = X0_grid.shape\r\n",
    "    X0_grid = np.expand_dims(X0_grid.flatten(), axis=1)\r\n",
    "    X1_grid = np.expand_dims(X1_grid.flatten(), axis=1)\r\n",
    "    grid = np.concatenate((X0_grid, X1_grid), axis=1)\r\n",
    "    grid_poly = poly_features.fit_transform(grid)\r\n",
    "    grid_preds = model.predict(grid_poly)\r\n",
    "    grid_preds = np.reshape(grid_preds, original_shape)\r\n",
    "    return grid_preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def visualize(X0_grid, X1_grid, grid_preds, X0, X1, Y, ax, c):\r\n",
    "    ax.set_xlim(xmin=np.min(X0), xmax=np.max(X0))\r\n",
    "    ax.set_ylim(ymin=np.min(X1), ymax=np.max(X1))\r\n",
    "    ax.set_zlim(zmin=np.min(Y), zmax=np.max(Y))\r\n",
    "    ax.set_xlabel('X0')\r\n",
    "    ax.set_ylabel('X1')\r\n",
    "    ax.set_zlabel('Target price')\r\n",
    "    ax.plot_surface(X0_grid, X1_grid, grid_preds, vmin=np.min(Y), vmax=np.max(Y), cmap=cm.coolwarm, linewidth=0, antialiased=False, alpha=0.5)\r\n",
    "    ax.scatter3D(X0, X1, Y, c=c, edgecolors='k', alpha=0.75, s=24)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def visualize_2D_polynomial_regression(model, poly_features, X_train, X_eval, Y_train, Y_eval, degree):\r\n",
    "    # Generate a grid of feature pairs:\r\n",
    "    X0_grid, X1_grid = make_grid(X0=np.concatenate((X_train[:,0], X_eval[:,0]), axis=0), \r\n",
    "                                 X1=np.concatenate((X_train[:,1], X_eval[:,1]), axis=0))\r\n",
    "    # Predict all feature pairs in the grid with the trained model:\r\n",
    "    grid_preds = predict_grid(model, poly_features, X0_grid, X1_grid)\r\n",
    "    # Display prediction surface and groundtruth for the train and the evaluation sets:\r\n",
    "    fig = plt.figure()\r\n",
    "    plt.suptitle(\"Prediction surface and groundtruth (degree = \" + str(degree) + \")\")\r\n",
    "    # Train set:\r\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection='3d')\r\n",
    "    ax1.set_title(\"Train set\")\r\n",
    "    visualize(X0_grid, X1_grid, grid_preds, X0=X_train[:,0], X1=X_train[:,1], Y=Y_train, ax=ax1, c='g')\r\n",
    "    # Evaluation_set:\r\n",
    "    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\r\n",
    "    ax2.set_title(\"Evaluation set\")\r\n",
    "    visualize(X0_grid, X1_grid, grid_preds, X0=X_eval[:,0], X1=X_eval[:,1], Y=Y_eval, ax=ax2, c='tab:orange')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit (windows store)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "f8dc394fe8b851e8fee8bf44b9060daf20dc28cf237837a1e07d4467514055a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}