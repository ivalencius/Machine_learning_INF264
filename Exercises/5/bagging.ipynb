{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Aggregation algorithm\n",
    "\n",
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset and store it as a pandas dataframe\n",
    "dataset = pd.read_csv('sonar.all-data',header=None)\n",
    "# Store features in a pandas dataframe X\n",
    "X = dataset.iloc[:,:-1]\n",
    "# Convert X into a numpy array\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Store labels in a numpy array y\n",
    "y = dataset.iloc[:,-1].to_numpy()\n",
    "# Convert it into a array of boolean (True if 'M' and False otherwise)\n",
    "y = (y == 'M')\n",
    "# Convert it into a array of int (1 if 'True' and 0 otherwise)\n",
    "y = y.astype(int)\n",
    "\n",
    "print(\"Number of samples: \", X.shape[0])\n",
    "print('Number of features: ', X.shape[-1])\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "### Subsample function\n",
    "\n",
    "\n",
    "You can use the function [random.choices()](https://docs.python.org/3/library/random.html#random.choices) to get a subsampling with replacement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def subsample(\n",
    "    X,\n",
    "    y,\n",
    "    n_samples=None   # number of sample in the subsampling\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a random subsample from the dataset with replacement\n",
    "    \"\"\"\n",
    "    ... #TODO!\n",
    "    return X_sample, y_sample\n",
    "\n",
    "def count_ratio_unique_subsample(X_sample):\n",
    "    ... #TODO!\n",
    "    return ratio"
   ]
  },
  {
   "source": [
    "### Bagging train and predict functions\n",
    "\n",
    "\n",
    "**Note**: in scikit-learn, all supervised estimators implement a ``fit(X, y)`` method and a ``predict(X)`` method with ``X`` being unlabeled observations and  ``y`` being labels. \n",
    "\n",
    "Therefore ``Classifier`` parameter can be any sklearn class implementing a supervised classifier.\n",
    "\n",
    "(See *The problem solved in supervised learning* section in the supervised learning tutorial from [sklearn documentation](https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_train(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    n_clfs,                                  # number of classifier\n",
    "    Classifier = DecisionTreeClassifier,     # Python class of classifier\n",
    "    clfs_args = {},                          # Specific python class of classifier's arguments\n",
    "):\n",
    "    \"\"\"\n",
    "    Bootstrap Aggregation training algorithm\n",
    "    \"\"\"\n",
    "    clfs = []\n",
    "    for i in range(n_clfs):\n",
    "        # -------------------------\n",
    "        # Train a new classifier\n",
    "        # -------------------------\n",
    "        # Take a subsample of X and Y (with replacement)\n",
    "        ... #TODO!\n",
    "        # Initialize a new Classifier object\n",
    "        ... #TODO!\n",
    "        # Train this new Classifier object\n",
    "        ... #TODO!\n",
    "        # Append your trained classifier in your list of classifiers \n",
    "        ... #TODO!\n",
    "    # Return the list of trained classifiers composing the bagging classifier\n",
    "    return clfs\n",
    "\n",
    "\n",
    "def bagging_predict(\n",
    "    clfs,     # list of classifiers composing the bagging classifier\n",
    "    X_test\n",
    "):\n",
    "    \"\"\"\n",
    "    Bootstrap Aggregation predict algorithm\n",
    "    \"\"\"\n",
    "    ... #TODO!\n",
    "    return y_pred\n"
   ]
  },
  {
   "source": [
    "### Cross validation\n",
    "\n",
    "\n",
    "\n",
    "You can use the [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) from sklearn to split your datasets into k folds. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold_split(X, y, num_folds, seed=int(666)):\n",
    "    \"\"\"\n",
    "    Split 'X' and 'y' into k-folds with k='num_fold'\n",
    "    \"\"\"\n",
    "    KFold_splitter = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "    # Initialize lists of X (train/val) and y (train/val) for each fold\n",
    "    X_train_folds = []  \n",
    "    X_val_folds = []\n",
    "    y_train_folds = []\n",
    "    y_val_folds = []\n",
    "    for (kth_fold_train_idxs, kth_fold_val_idxs) in KFold_splitter.split(X, y):\n",
    "        X_train_folds.append(X[kth_fold_train_idxs])\n",
    "        X_val_folds.append(X[kth_fold_val_idxs])\n",
    "        y_train_folds.append(y[kth_fold_train_idxs])\n",
    "        y_val_folds.append(y[kth_fold_val_idxs])\n",
    "    # Return the list of k-folds datasets\n",
    "    return X_train_folds, X_val_folds, y_train_folds, y_val_folds"
   ]
  },
  {
   "source": [
    "### Bagging evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bagging(\n",
    "    X_train_val, \n",
    "    y_train_val, \n",
    "    num_folds, \n",
    "    n_clfs,           # number of classifier\n",
    "    Classifier,       # Python class of classifier\n",
    "    clfs_args = {},   # Specific python class of classifier's arguments\n",
    "    seed=int(666),\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a cross validation of the bagging algorithm\n",
    "    \"\"\"\n",
    "    # Split train and validation datasets into k folds:\n",
    "    X_train_folds, X_val_folds, y_train_folds, y_val_folds = KFold_split(...) #TODO!\n",
    "    bagging_scores = []\n",
    "    bagging_clfs = []\n",
    "    \n",
    "    # For each set of k-folds get the bagging classifier and its accuracy\n",
    "    for X_train_fold, X_val_fold, y_train_fold, y_val_fold in zip(\n",
    "        X_train_folds, X_val_folds, y_train_folds, y_val_folds\n",
    "        ):\n",
    "        ... #TODO!\n",
    "    # Returns classifiers and validation scores of the bagging classifiers for each set of k folds\n",
    "    return bagging_clfs, bagging_scores"
   ]
  },
  {
   "source": [
    "### Test bagging on the sonar dataset\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "# Extract a test set:\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# For each hyper-parameter instance, do KFold cross validation:\n",
    "for n_trees in [1, 5, 10, 15, 20, 25, 30]:\n",
    "    bagging_clfs, bagging_scores = evaluate_bagging(...) #TODO!\n",
    "    )\n",
    "    print('Trees: %d' % n_trees)\n",
    "    print('Validation scores: ', [round(s, 3) for s in bagging_scores])\n",
    "    print('Mean validation accuracy: %.3f' % (sum(bagging_scores)/len(bagging_scores)))\n",
    "    # Test accuracy computed with the bagging classifier trained with the first k-fold\n",
    "    test_acc = accuracy_score(\n",
    "        bagging_predict(bagging_clfs[0], X_test),\n",
    "        y_test,\n",
    "    )\n",
    "    print('Test set accuracy: %.3f' %test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}